{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1a0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import InMemorySaver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "911df26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3354dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdais\\AppData\\Local\\Temp\\ipykernel_3824\\1205151563.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama2:7b\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama2:7b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef7ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "\n",
    "    topic:str\n",
    "    joke:str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd6d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(state: JokeState):\n",
    "\n",
    "    prompt = f'generate a joke on the topic {state[\"topic\"]}'\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    return {'joke': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b31fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation(state: JokeState):\n",
    "\n",
    "    prompt = f'write an explanation for the joke - {state[\"joke\"]}'\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    return {'explanation': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c59f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = StateGraph(JokeState)\n",
    "\n",
    "graph.add_node('generate_joke', generate_joke)\n",
    "graph.add_node('generate_explanation', generate_explanation)\n",
    "\n",
    "graph.add_edge(START, 'generate_joke')\n",
    "graph.add_edge('generate_joke', 'generate_explanation')\n",
    "graph.add_edge('generate_explanation', END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "workflow = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3293ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'pizza',\n",
       " 'joke': 'Why did the pizza go to therapy? It was feeling a little crusty!',\n",
       " 'explanation': '\\nAh, I see! The joke is a play on words, with \"crusty\" having a double meaning. In one sense, it can refer to the crispy exterior of a pizza crust. But in another sense, \"crusty\" can also mean grumpy or irritable, as in \"he\\'s feeling a little crusty today.\" So, the pizza is going to therapy because it\\'s feeling a bit grumpy or irritable, which is a clever and unexpected twist on the typical reason for seeing a therapist (e.g., dealing with personal issues). I hope that helps clarify things!'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "workflow.invoke({'topic':'pizza'}, config=config1) # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d576a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'pizza', 'joke': 'Why did the pizza go to therapy? It was feeling a little crusty!', 'explanation': '\\nAh, I see! The joke is a play on words, with \"crusty\" having a double meaning. In one sense, it can refer to the crispy exterior of a pizza crust. But in another sense, \"crusty\" can also mean grumpy or irritable, as in \"he\\'s feeling a little crusty today.\" So, the pizza is going to therapy because it\\'s feeling a bit grumpy or irritable, which is a clever and unexpected twist on the typical reason for seeing a therapist (e.g., dealing with personal issues). I hope that helps clarify things!'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092162-8e7c-6dbe-8002-21f94b16e23d'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-09-15T09:27:12.170131+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-fc92-64ca-8001-b93593d64f96'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_state(config1) # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f2793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'topic': 'pizza', 'joke': 'Why did the pizza go to therapy? It was feeling a little crusty!', 'explanation': '\\nAh, I see! The joke is a play on words, with \"crusty\" having a double meaning. In one sense, it can refer to the crispy exterior of a pizza crust. But in another sense, \"crusty\" can also mean grumpy or irritable, as in \"he\\'s feeling a little crusty today.\" So, the pizza is going to therapy because it\\'s feeling a bit grumpy or irritable, which is a clever and unexpected twist on the typical reason for seeing a therapist (e.g., dealing with personal issues). I hope that helps clarify things!'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092162-8e7c-6dbe-8002-21f94b16e23d'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-09-15T09:27:12.170131+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-fc92-64ca-8001-b93593d64f96'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'pizza', 'joke': 'Why did the pizza go to therapy? It was feeling a little crusty!'}, next=('generate_explanation',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-fc92-64ca-8001-b93593d64f96'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-09-15T09:26:56.869703+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-9147-64e8-8000-d6bed7057d18'}}, tasks=(PregelTask(id='6014d41e-be8c-d187-c922-5864b0a70f88', name='generate_explanation', path=('__pregel_pull', 'generate_explanation'), error=None, interrupts=(), state=None, result={'explanation': '\\nAh, I see! The joke is a play on words, with \"crusty\" having a double meaning. In one sense, it can refer to the crispy exterior of a pizza crust. But in another sense, \"crusty\" can also mean grumpy or irritable, as in \"he\\'s feeling a little crusty today.\" So, the pizza is going to therapy because it\\'s feeling a bit grumpy or irritable, which is a clever and unexpected twist on the typical reason for seeing a therapist (e.g., dealing with personal issues). I hope that helps clarify things!'}),), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'pizza'}, next=('generate_joke',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-9147-64e8-8000-d6bed7057d18'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-09-15T09:26:45.619223+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-913f-6f84-bfff-d57d4dd161d5'}}, tasks=(PregelTask(id='4d79378b-f364-a357-d12c-7ca81c9e5384', name='generate_joke', path=('__pregel_pull', 'generate_joke'), error=None, interrupts=(), state=None, result={'joke': 'Why did the pizza go to therapy? It was feeling a little crusty!'}),), interrupts=()),\n",
       " StateSnapshot(values={}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f092161-913f-6f84-bfff-d57d4dd161d5'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-09-15T09:26:45.616218+00:00', parent_config=None, tasks=(PregelTask(id='f0fde21a-c5a2-c0e5-609c-dbdf5703795b', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'topic': 'pizza'}),), interrupts=())]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(workflow.get_state_history(config1)) # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444d7286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'pasta',\n",
       " 'joke': 'Why did the spaghetti refuse to get dressed? Because it already had a saucy attitude!',\n",
       " 'explanation': '\\nThe joke \"Why did the spaghetti refuse to get dressed? Because it already had a saucy attitude!\" is a play on words that combines two different meanings of the word \"saucy.\"\\n\\nIn one sense, \"saucy\" can refer to something that is impudent or impertinent, as in \"The spaghetti was being saucy and refused to get dressed.\" In this context, the joke is funny because it takes a common phrase (\"refused to get dressed\") and gives it an unexpected twist by replacing the word \"refused\" with \"saucy,\" which has a different meaning.\\n\\nHowever, there is also a second layer of humor in the joke. In culinary terms, \"saucy\" can refer to food that is covered in sauce or gravy. So, the punchline \"because it already had a saucy attitude\" could be interpreted as the spaghetti being too coated in sauce to bother getting dressed. This double meaning adds an extra layer of humor to the joke.\\n\\nOverall, the joke is a clever play on words that uses two different meanings of \"saucy\" to create a humorous and unexpected punchline.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "workflow.invoke({'topic':'pasta'}, config=config2) # type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fd8b3",
   "metadata": {},
   "source": [
    "## Fault Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c774d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrashState(TypedDict):\n",
    "    input: str\n",
    "    step1: str\n",
    "    step2: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98843888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1(state: CrashState) -> CrashState:\n",
    "    print(\"✅ Step 1 executed\")\n",
    "    return {\"step1\": \"done\", \"input\": state[\"input\"]} # type:ignore \n",
    "\n",
    "def step_2(state: CrashState) -> CrashState:\n",
    "    print(\"⏳ Step 2 hanging... now manually interrupt from the notebook toolbar (STOP button)\")\n",
    "    time.sleep(1000)  # Simulate long-running hang\n",
    "    return {\"step2\": \"done\"} # type:ignore\n",
    "\n",
    "def step_3(state: CrashState) -> CrashState:\n",
    "    print(\"✅ Step 3 executed\")\n",
    "    return {\"done\": True} # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a44663fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the graph\n",
    "builder = StateGraph(CrashState)\n",
    "builder.add_node(\"step_1\", step_1)\n",
    "builder.add_node(\"step_2\", step_2)\n",
    "builder.add_node(\"step_3\", step_3)\n",
    "\n",
    "builder.set_entry_point(\"step_1\")\n",
    "builder.add_edge(\"step_1\", \"step_2\")\n",
    "builder.add_edge(\"step_2\", \"step_3\")\n",
    "builder.add_edge(\"step_3\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Running graph: Please manually interrupt during Step 2...\n",
      "✅ Step 1 executed\n",
      "⏳ Step 2 hanging... now manually interrupt from the notebook toolbar (STOP button)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"▶️ Running graph: Please manually interrupt during Step 2...\")\n",
    "    graph.invoke({\"input\": \"start\"}, config={\"configurable\": {\"thread_id\": 'thread-1'}}) # type:ignore\n",
    "except KeyboardInterrupt:\n",
    "    print(\"❌ Kernel manually interrupted (crash simulated).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ab6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Re-run to show fault-tolerant resume\n",
    "# print(\"\\n🔁 Re-running the graph to demonstrate fault tolerance...\")\n",
    "# final_state = graph.invoke(None, config={\"configurable\": {\"thread_id\": 'thread-1'}})\n",
    "# print(\"\\n✅ Final State:\", final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61714db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
